{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches\n",
    "\n",
    "If the result unique IDs could fit in the memory, this problem is easy to solve by read the input line by line, store the unique IDs in a `set` in memoery. After processing all the input IDs, write the this set to a file. \n",
    "\n",
    "However, if the memory is limited, we can still address this problem in the following different approaches.  \n",
    "- Linux Tools\n",
    "- Writing a Program\n",
    "- Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach I: Linux's **sort** and **uniq**\n",
    "\n",
    "The `sort` could do a [external sort](https://en.wikipedia.org/wiki/External_sorting) when the file is too large, and then `uniq` can get all the unique values.\n",
    "\n",
    "Here is the bash command.\n",
    "```bash\n",
    "$sort input_id.txt | uniq > uniq_id.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach II: Devide and Conquer (Python Program)\n",
    "\n",
    "**Algorithm**  \n",
    "1. Go through the whole input file, collect the unique IDs starting with a specific character (such as 'a').  \n",
    "2. Append these unique IDs in a the output file, or write to a speperate file.\n",
    "3. Continue from step 1 but choose another target leading char (such as 'b').\n",
    "4. End until all the leadding chars a-zA-Z0-9 has been processed.\n",
    "\n",
    "\n",
    "**Example**  \n",
    "\n",
    "```\n",
    "xNDGN3R\n",
    "8guP0Af\n",
    "VHgwqgA\n",
    "Wy0AXoo\n",
    "wy0AXoo\n",
    "xNDGN3R\n",
    "toS3f6T\n",
    "8bIgIXy\n",
    "xNDGN3R\n",
    "```\n",
    "\n",
    "Iteration 1: Read the file line by line, and collect all the uqique IDs start with 'a': None.\n",
    "...\n",
    "Iteration 20: Read the file line by line again,and collect all the uqique IDs start with 't': ('toS3f6T'). So, append it to the output.\n",
    "...\n",
    "Iteration X: ... process the uqique IDs start with 'x': ('xNDGN3R'). So, append it to the output file.\n",
    "Iteration 61: ...collect all the uqique IDs start with '8': ('8bIgIXy', '8guP0Af'), and append it to the output.\n",
    "...\n",
    "\n",
    "After 62 iterations, the output would be:\n",
    "```\n",
    "toS3f6T\n",
    "xNDGN3R\n",
    "VHgwqgA\n",
    "Wy0AXoo\n",
    "8bIgIXy\n",
    "8guP0Af\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implemention**\n",
    "\n",
    "Here is the python program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 3.6+\n",
    "\n",
    "import os\n",
    "import fileinput\n",
    "import collections\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "CHAR_SET = 'abcdefghijklmnopqrstuvwxyz' + 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' + '0123456789'\n",
    "\n",
    "INPUT_FILE = 'data/sample_id.txt' # default input file\n",
    "OUTPUT_FILE = 'uniq_id.txt' # default output file\n",
    "\n",
    "#Return a set of IDs started with `start_char` in file `file\n",
    "def extract_uniq_ids(input_file, output_file):\n",
    "    uniq_ids = set()\n",
    "\n",
    "    for char in CHAR_SET:\n",
    "        uniq_ids.clear()\n",
    "\n",
    "        start = datetime.now()\n",
    "        with fileinput.input(files = input_file) as f:\n",
    "            for line in f: # cannot use f.readlines() since the file is too large\n",
    "                line = line.strip()\n",
    "                if line.startswith(char) and line not in uniq_ids:\n",
    "                    uniq_ids.add(line)\n",
    "\n",
    "        if uniq_ids:\n",
    "            with open(output_file, 'a') as f: # write to seperate files if > 100MB\n",
    "                f.write('\\n'.join(uniq_ids))\n",
    "                f.write('\\n')\n",
    "\n",
    "        end = datetime.now()\n",
    "        print(f'Extracted {len(uniq_ids)} unique IDs starting with {char} using {end - start}.')\n",
    "    return uniq_ids\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    extract_uniq_ids(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitation**\n",
    "\n",
    "The success of the above algorithm replys on the volume of unique IDs starting with each character. If the data is highly skewed, certain initial char has so many unique IDs that could not be fit in the memoery, this program will fail due to out of memory issue. Developing other rules to 'partition' the data properly could fix this issue, such as taking the first and last char, or use string's hash value mod N (N is the times of the space/size of unique IDs / available memeory).\n",
    "\n",
    "Note:\n",
    "Sampling data could get a rough skewness reference, and `shuf -n x file` in Linux can get a random sample of x lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complexity and Optimization**\n",
    "\n",
    "The complexity is linear $O(N)$ because of scanning the big file 62 times, while 'N' is the total number of IDs in the input file. There is no way to reduce the complexity dramtically because you have to go through all the N IDs to find out all the unique IDs.\n",
    "\n",
    "However, we could process the huge file in batch mode and then combine these IDs with privious IDs stored in a file. The algorithm is decribed as below:\n",
    "\n",
    "1. Read several million lines each time, and collect the unique IDs grouped by the initial character.\n",
    "2. Save each group to a file, such as id-a.txt, id-b.txt, etc.\n",
    "3. Continue to process the next 10 million lines, and merge the group to previous group in the file.\n",
    "4. Proceed until it reaches the end of the file.\n",
    "5. Then append these small files to a new file to get all the unique IDs of the input file.\n",
    "\n",
    "In this way, the complexity could be reduced to $O(M*S)$, while 'M' is the number of unique IDs, and 'S' is the number of splits. (If you have 300 millions and process 10 million each time, the S is 30.) In this way, the huge file could only be read once, while the drawback is the intemediate small files will be read multiple times.\n",
    "\n",
    "Here is the optimized program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'id-'\n",
    "BATCH_NUM = 2000000\n",
    "INPUT_FILE = 'data/sample_id.txt'\n",
    "\n",
    "import os.path\n",
    "import fileinput\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def merge_ids(file, other_ids):\n",
    "    ids = set()\n",
    "\n",
    "    if os.path.isfile(file):\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line and line not in ids:\n",
    "                    ids.add(line)\n",
    "            f.close()\n",
    "\n",
    "    ids = ids.union(other_ids)\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        f.write('\\n'.join(ids))\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "\n",
    "    return True\n",
    "\n",
    "def extract_ids_batch(input_file, prefix = PREFIX, batch = BATCH_NUM):\n",
    "    m = defaultdict(set) # k:'[a-z|A-Z|0-9]', value = set()\n",
    "    with fileinput.input(files = input_file) as f:\n",
    "        while True:\n",
    "            m.clear()\n",
    "            for _ in range(BATCH_NUM):\n",
    "                line = f.readline()\n",
    "                if line == '': break # EOF\n",
    "                if line != '\\n': # skip the empty lines\n",
    "                    m[line[:1]].add(line.strip())\n",
    "\n",
    "            if not m:break\n",
    "\n",
    "            start = datetime.now()\n",
    "            for start_char, ids in m.items():\n",
    "                if start_char.isupper(): start_char += '_' # bugfix for case insensitive OS like mac\n",
    "\n",
    "                file_to_merge = prefix + start_char + '.txt'\n",
    "                merge_ids(file_to_merge, ids)\n",
    "            \n",
    "            end = datetime.now()\n",
    "            num = sum(len(m[ch]) for ch in m)\n",
    "            print(f'Merged {num} unique IDs using {end - start}.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    extract_ids_batch(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "The output are small files like id-0.txt, id-1.txt...id-_.txt, to merge all these files in a single file, use:  \n",
    "`cat id-*.txt > uniq_id_linux.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach III: Use database column's 'unique' feature\n",
    "\n",
    "**Intution**\n",
    "1. Take the same step as before to split the files into small files (using `split` in Linux).\n",
    "2. Create a table `unique_id` with one column `id` which is unique in MySQL.\n",
    "3. Load one file into the database table using `ignore` keword each time.\n",
    "4. Continue to process all the splitted files.  \n",
    "\n",
    "The reason why it works is because the duplicated values are ignored.   \n",
    "Here is the SQL code for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "mysql> create table if not EXISTS unique_id (id char(7) UNIQUE not NULL);\n",
    "mysql> load data infile '/var/lib/mysql/input_id.txt' ignore into table unique_id;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effenciency Comparation\n",
    "\n",
    "**Hardware**: AWS Lightsail (1 GB RAM, 1 vCPU, 40 GB SSD)  \n",
    "**Software**: Ubuntu 16.04.5 LTS (GNU/Linux 4.4.0-1074-aws x86_64)  \n",
    "**Free Memory**: 733MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profile 1\n",
    "\n",
    "**Data**\n",
    "\n",
    "Input File: data/sample_id.txt  \n",
    "Size:       7.7MB  \n",
    "Lines:      1000000 (1 million)  \n",
    "Output Size:7.6MB\n",
    "Uniq ID#:   990964  \n",
    "\n",
    "\n",
    "**Approaches**  \n",
    "\n",
    "| Approach                | Running Time  | Note                                   |\n",
    "| ----------------------- | ------------- | ---------------------------------------|\n",
    "| Linux `sort` and `uniq` | 1.9s          |                                        |\n",
    "| Programming             | 36s           |                                        |\n",
    "| Programming (Enhanced)  | 1.9s          | batch number: 200000 (20% of the input)|\n",
    "| *Database (MySQL)*      | 38s           |                                        |\n",
    "\n",
    "**Observation**  \n",
    "- The Linux command is most effecient, using 18x faster than the normal program way.  \n",
    "- The enhanced python program reduced the running time significantly, from 36s to 1.9s.  \n",
    "- The performance of the normal programming and database ways are the worst.  \n",
    "- Using database seems heave for this problem, so it will not be measured in later profiles.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profile 2\n",
    "\n",
    "**Data**  \n",
    "Input File : data/input_id.txt  \n",
    "Input Size:  100MB  \n",
    "Input Line#: 13107200\n",
    "Output Size: 93MB\n",
    "Unique ID#:  12073380\n",
    "\n",
    "**Approaches**  \n",
    "\n",
    "| Approach                | Running Time  | Note                                  |\n",
    "| ----------------------- | ------------- | ------------------------------------- |\n",
    "| Linux `sort` and `uniq` |  36s          |                                       |\n",
    "| Programming             |  488s     |                                       |\n",
    "| Programming (Enhanced)  |  40s      | batch size: 2000000 (~15%)            |\n",
    "| Programming (Enhanced)  |  500s     | batch size: 200000 (~1.5%)           |\n",
    "\n",
    "**Observation**   \n",
    "- The fastest appoach is still Linux's `sort` and `uniq` (And the result is in sort order as well).\n",
    "- The normal programming way imcrease linearly as the file size growths. The ratio of file size / running seconds is around 0.21MB/sec.\n",
    "- The enhanced programming way keeps pace with the Linux way with a high batch size (~15%).  \n",
    "- The performace of enhanced programming decreases when the batch number reduces, which could be as a proof of the time complexity of $O(M*S)$ as mentioned before.\n",
    "- The performance of normal program and enhanced program are similiar in case of small batch (~1.5%).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profile 3\n",
    "\n",
    "**Data**  \n",
    "Input File:  data/bigdata_id.txt  \n",
    "Size:        1.1GB  \n",
    "Line#:       134261128 (134 million)  \n",
    "Output Size: 519MB  \n",
    "Output ID#:  67998882  \n",
    "\n",
    "\n",
    "| Approach                | Running Time  | Note                                  |\n",
    "| ----------------------- | ------------- | ------------------------------------- |\n",
    "| Linux `sort` and `uniq` | 532s  |                         |\n",
    "| Programming             | 4836s | Estimated time according to 1'18'' * 62       |\n",
    "| Programming (Enhanced)  | n/a   | Batch number is 10 mln (~7%). Killed due to system OoM! |\n",
    "| Programming (Enhanced)  | 2951s | Batch number is 5 million.               |\n",
    "\n",
    "\n",
    "**Observation**  \n",
    "- The normal program processes ~0.23MB/s data liearly with the same rate as before.  \n",
    "- The enhanced program will be killed by operation system due to out of memory with too high batch number.\n",
    "- The enhanced program spent about 60% of the normal process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* Using Linux command (sort and uniq) is a effietient way to solve this unique ID problem.\n",
    "* The count each leading char program will process about 0.23MB/sec.\n",
    "* The enhanced program, which scann the input file only once, will boost the performance significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Data Generation\n",
    "\n",
    "The goal is to generate a about 1GB size files having IDs (a-z|A-Z|0-9), the unique IDs size of which is above 500MB.\n",
    "\n",
    "Quick estimation:\n",
    "- The lenght of a single ID is 7B.\n",
    "- The total ID (lines) in the generated file should be more than 130 millions.\n",
    "- The unique ID (lines) should be more than 65 millions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare a input ID file for demostration purpose: input_id.txt\n",
    "\n",
    "import random\n",
    "\n",
    "def gen_ids(valid_chars, length, N):\n",
    "    \"\"\"Generate N number of id:\n",
    "    - each char of the id is in valid_chars\n",
    "    - the lenght of id is length\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    for n in range(N):\n",
    "        id = random.choices(valid_chars, k=length)\n",
    "        ids.append(''.join(id))\n",
    "    return ids\n",
    "\n",
    "\n",
    "CHAR_SET = 'abcdefghijklmnopqrstuvwxyz' + 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' + '0123456789'\n",
    "INPUT_FILE = 'input_id.txt' # file for generated IDs\n",
    "ID_LENGTH = 7\n",
    "N = 10000000 # 10 million, run 7 times and then generate the rest of IDs by shuffle this fine using `shuf` command\n",
    "\n",
    "with open(INPUT_FILE, 'a') as f:\n",
    "    f.write('\\n'.join(gen_ids(CHAR_SET, ID_LENGTH, N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation\n",
    "\n",
    "Use this program to validate the format of IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated data/snippet_id.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File name: validate_id.py\n",
    "# Usage: python3 validate_id.py input_file.dat\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import fileinput\n",
    "\n",
    "ID_LENGTH = 7\n",
    "CHARS_SET = set('abcdefghijklmnopqrstuvwxyz' + 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' + '0123456789')\n",
    "\n",
    "def validate_id(path): # path is either file or dir\n",
    "    if os.path.isfile(path):\n",
    "        with fileinput.input(files=path) as f:\n",
    "            for line in f:\n",
    "                if line == os.linesep: continue # skip the blank lines\n",
    "                if line == '': break # reach the end of line\n",
    "                line = line.strip()\n",
    "                if not (line and len(line) == ID_LENGTH and set(line) <= CHARS_SET):\n",
    "                    print(f'Failed! Violation word:{line}')\n",
    "                    return False\n",
    "        print(f'Validated {path}')\n",
    "    elif os.path.isdir(path):\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                file = os.sep.join((root, file))\n",
    "                validate_id(file)\n",
    "    else:\n",
    "        print(f'File not exists: {path}')\n",
    "    return True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) > 1:\n",
    "        for argv in sys.argv[1:]:\n",
    "            validate_id(argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution of the first char (a-zA-Z0-9) the input file\n",
    "\n",
    "INPUT_FILE = 'data/sample_id.txt'\n",
    "\n",
    "import os\n",
    "import fileinput\n",
    "from collections import Counter\n",
    "\n",
    "with fileinput.input(files=INPUT_FILE) as f:    \n",
    "    counter = Counter(id.strip()[:1] for id in f if id and id != os.linesep)\n",
    "    for ch in sorted(counter):\n",
    "        print(ch, counter[ch])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
