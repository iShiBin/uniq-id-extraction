{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "The input file is around 2GB (~300 million lines), and there is just one commidity computer available(500MB free memory, 10GB disk space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches\n",
    "\n",
    "If the output unique IDs could fit in the memory, this problem is easy to solve by read the input in a small batch, then store the unique IDs in a `set`, and write the this set to a file after processing all the input IDs.\n",
    "\n",
    "However, this problem could be solved from different perspectives.\n",
    "- Linux Tools\n",
    "- Writing a Program\n",
    "- Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach I: Linux's **sort** and **uniq**\n",
    "\n",
    "The `sort` could do a [external sort](https://en.wikipedia.org/wiki/External_sorting) when the file is too large, and then `uniq` can get all the unique values.\n",
    "\n",
    "Here is the bash command.\n",
    "```bash\n",
    "$sort input_id.txt | uniq > uniq_id_bash.txtash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach II: Devide and Conquer (Python Program)\n",
    "\n",
    "**Algorithm**  \n",
    "Go through the input file multiple times, count the unique IDs starting with a specific character (such as 'a') each time. After done, append these unique IDs in a the output file, which is empty in the beginning.\n",
    "\n",
    "and then move to another one (such as 'b' etc.). \n",
    "\n",
    "**Example**  \n",
    "\n",
    "```\n",
    "xNDGN3R\n",
    "8guP0Af\n",
    "VHgwqgA\n",
    "Wy0AXoo\n",
    "wy0AXoo\n",
    "xNDGN3R\n",
    "toS3f6T\n",
    "8bIgIXy\n",
    "xNDGN3R\n",
    "```\n",
    "\n",
    "Iteration 1: Read the file line by line, and collect all the uqique IDs start with 'a': None. Do nothing since there is no unique ID starting with 'a'.\n",
    "...\n",
    "Iteration 20:Read the file line by line again,and collect all the uqique IDs start with 't': ('toS3f6T'). So, append it to the output (could be a file on disk).\n",
    "...\n",
    "Iteration... all the uqique IDs start with 'x': ('xNDGN3R'). So, append it to the output file.\n",
    "Iteration 61: Read the file line by line, and collect all the uqique IDs start with '8': ('8bIgIXy', '8guP0Af'), and append it to the output.\n",
    "Iteration 62: Read the file line by line, and collect all the uqique IDs start with '9': None\n",
    "\n",
    "After 62 iterations, the output would be:\n",
    "```\n",
    "toS3f6T\n",
    "xNDGN3R\n",
    "VHgwqgA\n",
    "Wy0AXoo\n",
    "8bIgIXy\n",
    "8guP0Af\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implemention**\n",
    "\n",
    "Here is the python program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 3.6+\n",
    "\n",
    "import os\n",
    "import fileinput\n",
    "import collections\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "CHAR_SET = 'abcdefghijklmnopqrstuvwxyz' + 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' + '0123456789'\n",
    "\n",
    "INPUT_FILE = 'input_id.txt' # default input file\n",
    "OUTPUT_FILE = 'uniq_id.txt' # default output file\n",
    "\n",
    "#Return a set of IDs started with `start_char` in file `file\n",
    "def extract_uniq_ids(input_file, output_file):\n",
    "    uniq_ids = set()\n",
    "    \n",
    "    for char in CHAR_SET:\n",
    "        uniq_ids.clear()\n",
    "        \n",
    "        start = datetime.now()\n",
    "        with fileinput.input(files = input_file) as f:\n",
    "            for line in f: # cannot use f.readlines() since the file is too large\n",
    "                line = line.strip()\n",
    "                if line.startswith(char) and line not in uniq_ids:\n",
    "                    uniq_ids.add(line)\n",
    "\n",
    "        if uniq_ids:\n",
    "            with open(output_file, 'a') as f:\n",
    "                f.write('\\n'.join(uniq_ids))\n",
    "                f.write('\\n')\n",
    "\n",
    "        end = datetime.now()\n",
    "        print(f'Extracted unique IDs starting with {char} using {end - start}.')\n",
    "    return uniq_ids\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    extract_uniq_ids(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitation**\n",
    "\n",
    "The success of the above algorithm replys on the volume of unique IDs starting with each character. If the data is highly skewed, certain initial char has so many unique IDs that could not be fit in the memoery, this program will fail due to out of memory issue. Developing other rules to 'partition' the data properly could fix this issue, such as taking the first and last char, or use string's hash value mod N (N is the times of the space/size of unique IDs / available memeory).\n",
    "\n",
    "Note:\n",
    "Sampling data could get a rough skewness reference, and `shuf -n x file` in Linux can get a random sample of x lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complexity and Optimization**\n",
    "\n",
    "The complexity is linear $O(N)$ because of scanning the big file 62 times, while 'N' is the total number of IDs in the input file. There is no way to reduce the complexity dramtically because you have to go through all the N IDs to find out all the unique IDs.\n",
    "\n",
    "However, if the number of unique IDs(M) is way less than the total IDs(N), we could process the huge file in batch mode and then combine these IDs with privious ID set. The algorithm is decribed as below:\n",
    "\n",
    "1. Read 10 million lines each time, and collect the unique IDs grouped by the initial character.\n",
    "2. Save each group to a file, such as id-a.txt, id-b.txt, etc.\n",
    "3. Continue to process the next 10 million lines, and merge the group to previous group in the file.\n",
    "4. Proceed until it reaches the end of the file.\n",
    "5. Then append these small files to a new file to get all the unique IDs of the input file.\n",
    "\n",
    "In this way, the complexity could be reduced to $O(M*S)$, while 'M' is the number of unique IDs, and 'S' is the number of splits. (If you have 300 millions and process 10 million each time, the S is 30.) In this way, the huge file could only be read once, while the trade off is the small files will be read multiple times.\n",
    "\n",
    "Here is the optimized implementation in case of only a relative small unique IDs volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = 'id-'\n",
    "BATCH_NUM = 10000000\n",
    "INPUT_FILE = 'input_id.txt'\n",
    "\n",
    "import os.path\n",
    "import fileinput\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def merge_ids(file, other_ids):\n",
    "    ids = set()\n",
    "    \n",
    "    if os.path.isfile(file): \n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line and line not in ids:\n",
    "                    ids.add(line)\n",
    "            f.close()\n",
    "\n",
    "    ids = ids.union(other_ids)\n",
    "    \n",
    "    with open(file, 'w') as f:\n",
    "        f.write('\\n'.join(ids))\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "\n",
    "    return True\n",
    "\n",
    "def extract_ids_batch(input_file, prefix = PREFIX, batch = BATCH_NUM):\n",
    "    m = defaultdict(set) # k:'[a-z|A-Z|0-9]', value = set()\n",
    "    with fileinput.input(files = input_file) as f:\n",
    "        while True:\n",
    "            m.clear()\n",
    "            for _ in range(BATCH_NUM):\n",
    "                line = f.readline()\n",
    "                if line and line != '\\n':\n",
    "                    m[line[:1]].add(line.strip())\n",
    "                else:top\n",
    "                    \n",
    "                    break\n",
    "\n",
    "            if not m:break\n",
    "            \n",
    "            for start_char, ids in m.items():\n",
    "                if start_char.isupper(): start_char += '_' # bugfix for case insensitive OS like mac\n",
    "                \n",
    "                file_to_merge = prefix + start_char + '.txt'\n",
    "                merge_ids(file_to_merge, ids)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    extract_ids_batch(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach III: Use database column's 'unique' feature\n",
    "\n",
    "**Intution**\n",
    "1. Take the same step as before to split the files into small files.\n",
    "2. Create a table `unique_id` with one column `id` which is unique in MySQL.\n",
    "3. Take one files and read all the IDs, then batch insert these values into the database using `ignore`.\n",
    "4. Continue to process all the splitted files till the end.\n",
    "\n",
    "The reason why it works is because the duplicated values are ignored. Here is the SQL code for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "create table if not EXISTS unique_id (id char(7) UNIQUE not NULL);\n",
    "load data infile '/var/lib/mysql/input_id.txt' ignore into table unique_id;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Data Generation\n",
    "\n",
    "The goal is to generate a about 2GB size files having IDs (a-z|A-Z|0-9), the unique IDs size of which is above 500MB.\n",
    "\n",
    "Quick estimation:\n",
    "- The lenght of a single ID is 7B.\n",
    "- The total ID (lines) in the generated file should be more than 300 millions.\n",
    "- The unique ID (lines) should be more than 70 millions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare a input ID file for demostration purpose: input_id.txt\n",
    "\n",
    "import random\n",
    "\n",
    "def gen_ids(valid_chars, length, N):\n",
    "    \"\"\"Generate N number of id:\n",
    "    - each char of the id is in valid_chars\n",
    "    - the lenght of id is length\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    for n in range(N):\n",
    "        id = random.choices(valid_chars, k=length)\n",
    "        ids.append(''.join(id))\n",
    "    return ids\n",
    "\n",
    "\n",
    "CHAR_SET = 'abcdefghijklmnopqrstuvwxyz' + 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' + '0123456789'\n",
    "INPUT_FILE = 'input_id.txt' # file for generated IDs\n",
    "ID_LENGTH = 7\n",
    "N = 30000000 # 30 millions, so run this for 6 times\n",
    "\n",
    "with open(INPUT_FILE, 'a') as f:\n",
    "    f.write('\\n'.join(gen_ids(CHAR_SET, ID_LENGTH, N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation\n",
    "\n",
    "Use this program to validate the format of IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the IDs\n",
    "\n",
    "import os\n",
    "import fileinput\n",
    "\n",
    "def validate_id(path): # path is either file or dir\n",
    "    if os.path.isfile(path):\n",
    "        with fileinput.input(files=path) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                if len(line) != ID_LENGTH:\n",
    "                    print(line)\n",
    "                    return False\n",
    "                for c in line:\n",
    "                    if not c in CHARS_SET:\n",
    "                        print(line)\n",
    "                        return False\n",
    "    else:\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                file = os.sep.join((root, file))\n",
    "                validate_id(file)\n",
    "    return True\n",
    "\n",
    "if '__name__' == '__main__':\n",
    "    validate_id(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effenciency Comparation\n",
    "\n",
    "**Input Data**  \n",
    "File Name: input_id.txt  \n",
    "Size:      2.0GB  \n",
    "Lines:     274 millions  \n",
    "\n",
    "**Output Date**  \n",
    "File Name: uniq_id.txt  \n",
    "Size:      540MB  \n",
    "Lines:     ~68 millions  \n",
    "\n",
    "**Running Machine Profile**  \n",
    "Hardware: AWS Lightsail (1 GB RAM, 1 vCPU, 40 GB SSD)  \n",
    "Software: Ubuntu 16.04.5 LTS (GNU/Linux 4.4.0-1074-aws x86_64)  \n",
    "\n",
    "*Available memory is around 400MB after startup*  \n",
    "\n",
    "\n",
    "**Approaches**  \n",
    "\n",
    "| Approach                | Running Time  | Note                                  |\n",
    "| ----------------------- | ------------- | ------------------------------------- |\n",
    "| Linux `sort` and `uniq` | < 20 min      | Most effecient                        |\n",
    "| Programming             | ~ 150 min     | ~2.5 min/each                         |\n",
    "| Programming (Enhanced)  |               |                                       |\n",
    "| Database (MySQL)        | > 60 min      | terminated mannuall due to 68 mln rows|\n",
    "\n",
    "Note:  \n",
    "- In the programming approach, the performace degrade to 1/10 after around 'k' (inital char), so suggest to output to each individual files rather than appending to a huge file.  \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
